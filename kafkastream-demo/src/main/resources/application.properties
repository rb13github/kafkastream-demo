
#server.port=8777


logging.level.root=info
#logging.level.org.springframework=INFO
#logging.level.org.springframework.web=INFO
#logging.level.org.hibernate=ERROR


#spring.datasource.url=jdbc:postgresql://localhost:5432/postgres
#spring.datasource.username=postgres
#spring.datasource.password=postgres
#spring.jpa.generate-ddl=true

!-- using environment variable
spring.datasource.url=${DB_URL}
spring.datasource.username=${DB_USER}
spring.datasource.password=${DB_PASSWORD}
spring.jpa.properties.hibernate.dialect = org.hibernate.dialect.PostgreSQLDialect
spring.jpa.hibernate.ddl-auto=none
!--spring.jpa.hibernate.show-sql=true
spring.jpa.database-platform=org.hibernate.dialect.PostgreSQLDialect


#kafka.bootstrapAddress=localhost:9092
kafka.bootstrapAddress=${KAFKA_BOOTSTRAP_URL}
#message.topic.product-reserve-consume.name=oms-reserve-event
#message.topic.product-status-publish.name=oms-product-status
#filtered.topic.name=filtered
#partitioned.topic.name=partitioned
#zookeeper.product-reserve-consume.group-id=reservegroupid
#zookeeper.product-status-publish.group-id=statusgroupid

#cloud.stream.kafka.binder.brokers=localhost:9092
cloud.stream.kafka.binder.brokers=${KAFKA_BROKERS_URL}
cloud.stream.kafka.binder.auto-create-topics= true
#cloud.stream.kafka.binder.auto-create-topics= ${AUTO_CREATE_TOPICS}

cloud.stream.kafka.binder.auto-add-partitions= true
#cloud.stream.kafka.binder.auto-add-partitions= ${AUTO_ADD_PARTITIONS}

 
 #Producer 
 #bindings.product-status-producer.destination=oms-product-status
bindings.product-status-producer.destination=${KAFKA_TOPIC_OMS_PRODUCT_STATUS}
  bindings.product-status-producer.content-type=application/json  
  
  
  #consumer
 # bindings.product-reserve-consumer.destination=oms-reserve-event
 bindings.product-reserve-consumer.destination=${KAFKA_TOPIC_OMS_RESERVE_EVENT}
  bindings.product-reserve-consumer.content-type=application/json  